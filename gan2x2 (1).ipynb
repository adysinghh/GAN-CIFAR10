{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NyjBKiLmTA5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_eIR4te-JqL"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Function to generate a single 32x32 RGB image with unique values for each corner\n",
        "def generate_image():\n",
        "    # Initialize a 32x32 image with 3 color channels (RGB)\n",
        "    image = np.zeros((32, 32, 3), dtype=np.uint8)\n",
        "\n",
        "    # Generate unique random values for each corner\n",
        "    dark_value1 = np.random.uniform(0, 0.5)  # Dark value between 0 and 0.5\n",
        "    dark_value2 = np.random.uniform(0, 0.5)  # Another unique dark value\n",
        "    light_value1 = np.random.uniform(0.5, 1)  # Light value between 0.5 and 1\n",
        "    light_value2 = np.random.uniform(0.5, 1)  # Another unique light value\n",
        "\n",
        "    # Ensure that dark values are different from each other\n",
        "    while dark_value2 == dark_value1:\n",
        "        dark_value2 = np.random.uniform(0, 0.5)\n",
        "\n",
        "    # Ensure that light values are different from each other\n",
        "    while light_value2 == light_value1:\n",
        "        light_value2 = np.random.uniform(0.5, 1)\n",
        "\n",
        "    # Create 16x16 patches with these values\n",
        "    dark_patch1 = np.full((16, 16, 3), dark_value1 * 255, dtype=np.uint8)\n",
        "    dark_patch2 = np.full((16, 16, 3), dark_value2 * 255, dtype=np.uint8)\n",
        "    light_patch1 = np.full((16, 16, 3), light_value1 * 255, dtype=np.uint8)\n",
        "    light_patch2 = np.full((16, 16, 3), light_value2 * 255, dtype=np.uint8)\n",
        "\n",
        "    # Place the patches in the appropriate positions\n",
        "    image[:16, :16] = dark_patch1  # Top-left corner\n",
        "    image[16:, 16:] = dark_patch2  # Bottom-right corner\n",
        "    image[:16, 16:] = light_patch1  # Top-right corner\n",
        "    image[16:, :16] = light_patch2  # Bottom-left corner\n",
        "\n",
        "    return image\n",
        "\n",
        "# Function to create a dataset\n",
        "def create_dataset(num_images, save_dir):\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "\n",
        "    for i in range(num_images):\n",
        "        image = generate_image()\n",
        "        img = Image.fromarray(image, 'RGB')\n",
        "        img.save(os.path.join(save_dir, f'image_{i}.png'))\n",
        "\n",
        "# Number of images to generate\n",
        "num_images = 1000  # Adjust as needed\n",
        "save_dir = 'synthetic_dataset'\n",
        "\n",
        "# Generate and save the dataset\n",
        "create_dataset(num_images, save_dir)\n",
        "\n",
        "print(f'Dataset generated and saved in {save_dir}')\n"
      ],
      "metadata": {
        "id": "-TR3U0Zz-T5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load the images into memory\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def load_dataset(dataset_dir, img_size=(32, 32)):\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    for filename in os.listdir(dataset_dir):\n",
        "        if filename.endswith('.png'):\n",
        "            # Load image and convert to RGB\n",
        "            img_path = os.path.join(dataset_dir, filename)\n",
        "            img = Image.open(img_path).convert('RGB')\n",
        "            img = img.resize(img_size)  # Resize image\n",
        "            img_array = np.array(img, dtype=np.float32) / 255.0  # Normalize to [0, 1]\n",
        "\n",
        "            images.append(img_array)\n",
        "\n",
        "            label = 1\n",
        "            labels.append(label)\n",
        "\n",
        "    images = np.array(images)\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    return images, labels\n",
        "\n",
        "# Load dataset\n",
        "dataset_dir = 'synthetic_dataset'\n",
        "images, labels = load_dataset(dataset_dir)\n",
        "\n",
        "trainX, testX, trainY, testY = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Training images shape: {trainX.shape}\")\n",
        "print(f\"Testing images shape: {testX.shape}\")\n",
        "print(f\"Training labels shape: {trainY.shape}\")\n",
        "print(f\"Testing labels shape: {testY.shape}\")\n"
      ],
      "metadata": {
        "id": "9OYZF3axAIcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-vis"
      ],
      "metadata": {
        "collapsed": true,
        "id": "kN3GKzrhEzvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot images from the training dataset\n",
        "num_images = 49  # Number of images to display\n",
        "plt.figure(figsize=(10, 10))  # fig size\n",
        "\n",
        "for i in range(min(num_images, len(trainX))):\n",
        "    # Define subplot\n",
        "    plt.subplot(7,7,1+i)\n",
        "    # Turn off axis\n",
        "    plt.axis('off')\n",
        "    # Plot raw pixel data\n",
        "    plt.imshow(trainX[i])\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DY85kQ67E5I5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discriminator Model"
      ],
      "metadata": {
        "id": "cQHRMkUqFmbe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#DEFINING DISCRIMINATOR MODEL\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LeakyReLU\n",
        "from tensorflow.keras.utils import plot_model\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "HqxMx6OkGE5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Disriminator model\n",
        "def define_discriminator(in_shape=(32,32,3)):\n",
        "  model = Sequential()\n",
        "  #normal\n",
        "  model.add(Conv2D(64,(3,3),padding='same',input_shape=in_shape))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  #downsample\n",
        "  model.add(Conv2D(128,(3,3),strides=(2,2),padding=\"same\"))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  #downsample\n",
        "  model.add(Conv2D(128,(3,3),strides=(2,2),padding='same'))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  #downsample\n",
        "  model.add(Conv2D(256,(3,3),strides=(2,2),padding='same'))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  #classifier\n",
        "  model.add(Flatten())\n",
        "  model.add(Dropout(0.4))\n",
        "  model.add(Dense(1,activation='sigmoid'))\n",
        "  #compile model\n",
        "  opt = Adam(learning_rate=0.0002,beta_1=0.5)\n",
        "  model.compile(loss='binary_crossentropy',optimizer=opt,metrics=['accuracy'])\n",
        "  return model\n"
      ],
      "metadata": {
        "id": "6-vArdyrGGkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define model\n",
        "model = define_discriminator()\n",
        "#summarize the model\n",
        "model.summary()\n",
        "#Plot the model\n",
        "plot_model(model, to_file='discriminator_plot.png',show_shapes=True,show_layer_names=True)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "NPXDc8lOGKmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_real_samples():\n",
        "    # Load dataset\n",
        "    trainX, _ = load_dataset(dataset_dir='synthetic_dataset')\n",
        "    # Convert from unsigned ints to float\n",
        "    X = trainX.astype('float32')\n",
        "    # Scale from [0, 1] to [-1, 1]\"\"\n",
        "    X = (X - 0.5) / 0.5\n",
        "    return X\n"
      ],
      "metadata": {
        "id": "a0bXChL4GUvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_shape = load_real_samples()\n",
        "x_shape.shape\n",
        "x_shape[0]"
      ],
      "metadata": {
        "id": "YI5u8QdqGffU",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#select half(64) form real samples\n",
        "def generate_real_samples(dataset, n_samples):\n",
        "  #choose random instances\n",
        "  ix = np.random.randint(0, dataset.shape[0], n_samples)\n",
        "  #retrieve selected images\n",
        "  X = dataset[ix]\n",
        "  #generate 'real' class labels(1)\n",
        "  y = np.ones((n_samples, 1))\n",
        "  return X, y"
      ],
      "metadata": {
        "id": "DOcU1APJGmI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_real_samples()\n",
        "\n",
        "X_real,y_real = generate_real_samples(dataset, 64)\n",
        "plt.imshow(X_real[0])\n"
      ],
      "metadata": {
        "id": "ENFFG2wpIcjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_real_samples()\n",
        "\n",
        "X_real,y_real = generate_real_samples(dataset, 64)\n",
        "plt.imshow(X_real[0])\n"
      ],
      "metadata": {
        "id": "MEGc1WOSb6YX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#generate fake samples with class labels\n",
        "def generate_fake_samples(n_samples):\n",
        "  #generate uniform random numbers in [0,1]\n",
        "  X = np.random.rand(32*32*3*n_samples)\n",
        "  #update to have the range [-1,1]\n",
        "  X = -1 + X * 2\n",
        "  #reshape into a batch of color images\n",
        "  X = X.reshape((n_samples, 32, 32, 3))\n",
        "  #generate 'fake' class lables (0)\n",
        "  y = np.zeros((n_samples, 1))\n",
        "  return X,y"
      ],
      "metadata": {
        "id": "DC6zMBRwIejb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_fake,y_fake = generate_fake_samples(64)\n",
        "plt.imshow(X_fake[0])"
      ],
      "metadata": {
        "id": "Pg6Uv25uLEZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_fake.shape) #(64, 32, 32, 3)\n",
        "print(y_fake.shape) #(64, 32, 32, 3)\n",
        "print(y_fake) #[[0.].."
      ],
      "metadata": {
        "id": "tBmkBNKiLGOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train The Discriminator model temporaily\n",
        "def train_discriminator(model, dataset, n_epochs=40, n_batch=128):\n",
        "  half_batch = int(n_batch / 2)\n",
        "  #manually enumerate epochs\n",
        "  for i in range(n_epochs):\n",
        "    #get randomly selected 'real' samples\n",
        "    X_real1,y_real1 = generate_real_samples(dataset, half_batch)\n",
        "    #update discriminator on real samples\n",
        "    _, real_acc = model.train_on_batch(X_real1, y_real1)\n",
        "    #generate 'fake' examples\n",
        "    X_fake1, y_fake1 = generate_fake_samples(half_batch)\n",
        "    #update discriminator on fake samples\n",
        "    _, fake_acc = model.train_on_batch(X_fake, y_fake)\n",
        "    #summarize performance\n",
        "    print('>%d real=%.0f%% fake=%.0f%%' % (i+1,real_acc*100,fake_acc*100))\n"
      ],
      "metadata": {
        "id": "abtE4It5LKnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Example Training\n",
        "\n",
        "#define the discriminator model\n",
        "model = define_discriminator()\n",
        "#load image data\n",
        "dataset = load_real_samples()\n",
        "#fit the model\n",
        "train_discriminator(model, dataset)"
      ],
      "metadata": {
        "id": "j9qwmfPULPcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#GENERATOR"
      ],
      "metadata": {
        "id": "w-tX2eskLUaE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Reshape\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import Conv2DTranspose\n",
        "from keras.layers import LeakyReLU\n",
        "from tensorflow.keras.utils import plot_model\n"
      ],
      "metadata": {
        "id": "SMCJGXP7Lvs4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define the GENERATOR MODEL\n",
        "def define_generator(latent_dim):\n",
        "  model = Sequential()\n",
        "  # foundation for 4x4 image\n",
        "  n_nodes = 256 * 4 * 4\n",
        "  model.add(Dense(4096, input_dim = latent_dim))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(Reshape((4, 4, 256)))\n",
        "  # unsample to 8x8\n",
        "  model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  # unsample to 16x16\n",
        "  model.add(Conv2DTranspose(128, (4,4), strides=(2,2),padding='same'))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  # unsample to 32x32\n",
        "  model.add(Conv2DTranspose(128, (4,4), strides=(2,2),padding='same'))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  #output layer\n",
        "  model.add(Conv2D(3, (3,3), activation='tanh', padding='same'))\n",
        "  return model"
      ],
      "metadata": {
        "id": "nen1v7esLxbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define size of latent space (NOISE)\n",
        "latent_dim = 100\n",
        "# define the generator model\n",
        "model = define_generator(latent_dim)\n",
        "#summarize the model\n",
        "model.summary()\n",
        "#plot the model\n",
        "plot_model(model, to_file='generator_plot.png', show_shapes=True, show_layer_names =True)"
      ],
      "metadata": {
        "id": "K7ckDQZ3L0lI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#generate points in latent space as input for the generator\n",
        "def generate_latent_points(latent_dim, n_samples):\n",
        "  #generate points in the latent space\n",
        "  x_input = np.random.randn(latent_dim * n_samples)\n",
        "  #reshape into a batch of inputs for the network\n",
        "  x_input = x_input.reshape(n_samples, latent_dim)\n",
        "  return x_input"
      ],
      "metadata": {
        "id": "VYLjuQt_L2aI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_input = generate_latent_points(100, 64)\n",
        "print(x_input.shape)"
      ],
      "metadata": {
        "id": "P0sksra6L7L7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#use the generator to generate n fake samples, with class labels\n",
        "def generate_fake_samples(g_model, latent_dim, n_samples):\n",
        "  #generate points in latent space\n",
        "  x_input = generate_latent_points(latent_dim, n_samples)\n",
        "  #predict outputs\n",
        "  X = g_model.predict(x_input)\n",
        "  #create 'fake' class labels (0)\n",
        "  y = np.zeros((n_samples, 1))\n",
        "  return X, y"
      ],
      "metadata": {
        "id": "wt7yggXWL9qK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#size of latent space\n",
        "latent_dim = 100\n",
        "#define the generator model\n",
        "model = define_generator(latent_dim)\n",
        "#generate samples\n",
        "n_samples = 49\n",
        "X, _ = generate_fake_samples(model, latent_dim, n_samples)\n",
        "#scale pixel values from [-1,1] to [0,1]\n",
        "X = (X+1) / 2.0\n",
        "#Plot the generated samples\n",
        "for i in range(n_samples):\n",
        "  #define subplot\n",
        "  plt.subplot(7, 7 , 1+i)\n",
        "  #turn off axis labels\n",
        "  plt.axis('off')\n",
        "  #plot single image\n",
        "  plt.imshow(X[i])\n",
        "#show the figure\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6slw7D7bMAI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define the combines generator and discriminator model, for updating the genertor\n",
        "def define_gan(g_model, d_model):\n",
        "  #make weights in the discriminator not trainable\n",
        "  d_model.trainable = False\n",
        "  #connect them\n",
        "  model = Sequential()\n",
        "  #add generator\n",
        "  model.add(g_model)\n",
        "  #add the discriminator\n",
        "  model.add(d_model)\n",
        "  #comiple model\n",
        "  opt = Adam(learning_rate=0.0002, beta_1 = 0.5)\n",
        "  model.compile(loss='binary_crossentropy', optimizer = opt)\n",
        "  return model"
      ],
      "metadata": {
        "id": "kHHl-dA_MCMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#size of the latent space\n",
        "latent_dim = 100\n",
        "#create the discriminator\n",
        "d_model = define_discriminator()\n",
        "#create the generator\n",
        "g_model = define_generator(latent_dim)\n",
        "#create the gan\n",
        "gan_model = define_gan(g_model, d_model)\n",
        "#summarize gan model\n",
        "gan_model.summary()\n",
        "#plot GAN MODEL\n",
        "plot_model(gan_model,to_file='gan_plot.png', show_shapes=True, show_layer_names=True)"
      ],
      "metadata": {
        "id": "E_upFKkwMFZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=100, n_batch=32):\n",
        "    batch_per_epoch = int(dataset.shape[0] / n_batch)\n",
        "    half_batch = int(n_batch / 2)\n",
        "\n",
        "    # Manually enumerate epochs\n",
        "    for i in range(n_epochs):\n",
        "        # Enumerate batches over the training set\n",
        "        for j in range(batch_per_epoch):\n",
        "            # Get randomly selected 'real' samples\n",
        "            X_real, y_real = generate_real_samples(dataset, half_batch)\n",
        "            # Update discriminator model weights\n",
        "            d_loss1, _ = d_model.train_on_batch(X_real, y_real)\n",
        "\n",
        "            # Generate 'fake' examples\n",
        "            X_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
        "            # Update discriminator model weights\n",
        "            d_loss2, _ = d_model.train_on_batch(X_fake, y_fake)\n",
        "\n",
        "            # Prepare points in latent space as input for the generator\n",
        "            X_gan = generate_latent_points(latent_dim, n_batch)\n",
        "            # Create inverted labels for the fake samples\n",
        "            y_gan = np.ones((n_batch, 1))\n",
        "\n",
        "            # Update the generator via the discriminator's error\n",
        "            g_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
        "\n",
        "            # Ensure g_loss is a scalar\n",
        "            if isinstance(g_loss, list):\n",
        "                g_loss_value = g_loss[0]\n",
        "            else:\n",
        "                g_loss_value = g_loss\n",
        "\n",
        "            # Summarize loss on this batch\n",
        "            print(f'>{i+1}, {j+1}/{batch_per_epoch}, d1={d_loss1:.3f}, d2={d_loss2:.3f}, g={g_loss_value:.3f}')\n",
        "\n",
        "            # Evaluate the model performance, sometimes\n",
        "            if (i+1) % 2 == 0:\n",
        "                summarize_performance(i, g_model, d_model, dataset, latent_dim)\n"
      ],
      "metadata": {
        "id": "BKU-RJMeMG9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_performance(epoch, g_model, d_model, dataset, latent_dim, n_samples=150):\n",
        "    # Prepare real samples\n",
        "    X_real, y_real = generate_real_samples(dataset, n_samples)\n",
        "    # Evaluate discriminator on real examples\n",
        "    _, acc_real = d_model.evaluate(X_real, y_real, verbose=0)\n",
        "    # Prepare fake examples\n",
        "    X_fake, y_fake = generate_fake_samples(g_model, latent_dim, n_samples)\n",
        "    # Evaluate discriminator performance on fake examples\n",
        "    _, acc_fake = d_model.evaluate(X_fake, y_fake, verbose=0)\n",
        "    # Print accuracy\n",
        "    print(f'>Accuracy real: {acc_real*100:.2f}%, fake: {acc_fake*100:.2f}%')\n",
        "    # Save plot\n",
        "    save_plot(X_fake, epoch)\n",
        "    # Save the generator model as an H5 file\n",
        "    filename = f'generator_model_{epoch+1:03d}.h5'\n",
        "    g_model.save(filename)\n"
      ],
      "metadata": {
        "id": "3fNCWy0xMJyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create and save a plot of generated images\n",
        "def save_plot(examples, epoch, n=7):\n",
        "  #scale form [-1,1] to [0,1]\n",
        "  examples = (examples + 1) / 2.0\n",
        "  #plot images\n",
        "  for i in range(n * n):\n",
        "    #define subplot\n",
        "    plt.subplot(n, n, 1+i)\n",
        "    #turn off axis\n",
        "    plt.axis('off')\n",
        "    #plot raw pixel data\n",
        "    plt.imshow(examples[i])\n",
        "  #save plot to file\n",
        "  filename = 'generated_plot_e%03d.png' % (epoch+1)\n",
        "  plt.savefig(filename)\n",
        "  plt.close()"
      ],
      "metadata": {
        "id": "6OifqusbMMer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=100, n_batch=32)"
      ],
      "metadata": {
        "id": "EcWsTGLmMOhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#example of loading the generator model and generating images\n",
        "from keras.models import load_model\n",
        "from numpy.random import randn\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "#generate points in latent space as input for the generator\n",
        "def generate_latent_points(latent_dim, n_samples):\n",
        "  #generate points in the latent space\n",
        "  x_input = randn(latent_dim * n_samples)\n",
        "  #reshape into a batch of inputs for the network\n",
        "  x_input = x_input.reshape(n_samples, latent_dim)\n",
        "  return x_input\n",
        "\n",
        "#plot the generated images\n",
        "def create_plot(examples, n):\n",
        "  #plot images\n",
        "  for i in range(n * n):\n",
        "    #define subplot\n",
        "    plt.subplot(n, n, 1+i)\n",
        "    #turn off axis\n",
        "    plt.axis('off')\n",
        "    #plot raw pixel data\n",
        "    plt.imshow(examples[i, :, :])\n",
        "  plt.show()\n",
        "\n",
        "#load model\n",
        "model = load_model('generator_model_002.h5')\n",
        "#generator images\n",
        "latent_points = generate_latent_points(100, 100)\n",
        "#generate images\n",
        "X = model.predict(latent_points)\n",
        "#scale from [-1,1] to [0,1]\n",
        "X = (X+1) / 2.0\n",
        "#plot the result\n",
        "create_plot(X, 1)\n",
        "\n",
        "X.shape[0]"
      ],
      "metadata": {
        "id": "Uo-ptLD6MRA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=2, n_batch=32):\n",
        "    batch_per_epoch = int(dataset.shape[0] / n_batch)\n",
        "    half_batch = int(n_batch / 2)\n",
        "\n",
        "    # Manually enumerate epochs\n",
        "    for i in range(n_epochs):\n",
        "        # Enumerate batches over the training set\n",
        "        for j in range(batch_per_epoch):\n",
        "            # Get randomly selected 'real' samples\n",
        "            X_real, y_real = generate_real_samples(dataset, half_batch)\n",
        "            # Update discriminator model weights\n",
        "            d_loss1, _ = d_model.train_on_batch(X_real, y_real)\n",
        "\n",
        "            # Generate 'fake' examples\n",
        "            X_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
        "            # Update discriminator model weights\n",
        "            d_loss2, _ = d_model.train_on_batch(X_fake, y_fake)\n",
        "\n",
        "            # Prepare points in latent space as input for the generator\n",
        "            X_gan = generate_latent_points(latent_dim, n_batch)\n",
        "            # Create inverted labels for the fake samples\n",
        "            y_gan = np.ones((n_batch, 1))\n",
        "\n",
        "            # Update the generator via the discriminator's error\n",
        "            g_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
        "\n",
        "            # Ensure g_loss is a scalar\n",
        "            if isinstance(g_loss, list):\n",
        "                g_loss_value = g_loss[0]\n",
        "            else:\n",
        "                g_loss_value = g_loss\n",
        "\n",
        "            # Summarize loss on this batch\n",
        "            print(f'>{i+1}, {j+1}/{batch_per_epoch}, d1={d_loss1:.3f}, d2={d_loss2:.3f}, g={g_loss_value:.3f}')\n",
        "\n",
        "            # Evaluate the model performance, sometimes\n",
        "            if (i+1) % 10 == 0:\n",
        "                summarize_performance(i, g_model, d_model, dataset, latent_dim)\n"
      ],
      "metadata": {
        "id": "Zpk3NbCPQx9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow keras\n"
      ],
      "metadata": {
        "id": "gHrCKP6e1rWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#example of loading the generator model and generating images\n",
        "from keras.models import load_model\n",
        "from numpy.random import randn\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "#generate points in latent space as input for the generator\n",
        "def generate_latent_points(latent_dim, n_samples):\n",
        "  #generate points in the latent space\n",
        "  x_input = randn(latent_dim * n_samples)\n",
        "  #reshape into a batch of inputs for the network\n",
        "  x_input = x_input.reshape(n_samples, latent_dim)\n",
        "  return x_input\n",
        "\n",
        "#plot the generated images\n",
        "def create_plot(examples, n):\n",
        "  #plot images\n",
        "  for i in range(n * n):\n",
        "    #define subplot\n",
        "    plt.subplot(n, n, 1+i)\n",
        "    #turn off axis\n",
        "    plt.axis('off')\n",
        "    #plot raw pixel data\n",
        "    plt.imshow(examples[i, :, :])\n",
        "  plt.show()\n",
        "\n",
        "#load model\n",
        "model = load_model('generator_model_052.h5')\n",
        "#generator images\n",
        "latent_points = generate_latent_points(100, 100)\n",
        "#generate images\n",
        "X = model.predict(latent_points)\n",
        "#scale from [-1,1] to [0,1]\n",
        "X = (X+1) / 2.0\n",
        "#plot the result\n",
        "create_plot(X, 1)\n",
        "\n",
        "X.shape[0]"
      ],
      "metadata": {
        "id": "NmBNBZ_Ivi4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#example of loading the generator model and generating images\n",
        "from keras.models import load_model\n",
        "from numpy.random import randn\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "#generate points in latent space as input for the generator\n",
        "def generate_latent_points(latent_dim, n_samples):\n",
        "  #generate points in the latent space\n",
        "  x_input = randn(latent_dim * n_samples)\n",
        "  #reshape into a batch of inputs for the network\n",
        "  x_input = x_input.reshape(n_samples, latent_dim)\n",
        "  return x_input\n",
        "\n",
        "#plot the generated images\n",
        "def create_plot(examples, n):\n",
        "  #plot images\n",
        "  for i in range(n * n):\n",
        "    #define subplot\n",
        "    plt.subplot(n, n, 1+i)\n",
        "    #turn off axis\n",
        "    plt.axis('off')\n",
        "    #plot raw pixel data\n",
        "    plt.imshow(examples[i, :, :])\n",
        "  plt.show()\n",
        "\n",
        "#load model\n",
        "model = load_model('generator_model_002.h5')\n",
        "#generator images\n",
        "latent_points = generate_latent_points(100, 100)\n",
        "#generate images\n",
        "X = model.predict(latent_points)\n",
        "#scale from [-1,1] to [0,1]\n",
        "X = (X+1) / 2.0\n",
        "#plot the result\n",
        "create_plot(X, 1)\n",
        "\n",
        "X.shape[0]"
      ],
      "metadata": {
        "id": "2B4CsnCKvkr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l6nTe06AF56g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}